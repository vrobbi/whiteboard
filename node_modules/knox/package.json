{
  "name": "knox",
  "description": "Amazon S3 client",
  "keywords": [
    "aws",
    "amazon",
    "s3"
  ],
  "version": "0.3.1",
  "author": {
    "name": "TJ Holowaychuk",
    "email": "tj@learnboost.com"
  },
  "contributors": [
    {
      "name": "TJ Holowaychuk",
      "email": "tj@learnboost.com"
    },
    {
      "name": "Domenic Denicola",
      "email": "domenic@domenicdenicola.com"
    }
  ],
  "license": "MIT",
  "main": "./lib/index.js",
  "repository": {
    "type": "git",
    "url": "git://github.com/LearnBoost/knox.git"
  },
  "bugs": {
    "url": "https://github.com/LearnBoost/knox/issues"
  },
  "dependencies": {
    "mime": "*"
  },
  "devDependencies": {
    "mocha": "*"
  },
  "scripts": {
    "test": "mocha --slow 500ms --reporter spec"
  },
  "directories": {
    "lib": "./lib"
  },
  "readme": "# knox\r\n\r\nNode Amazon S3 Client.\r\n\r\n## Features\r\n\r\n  - Familiar API (`client.get()`, `client.put()`, etc)\r\n  - Very Node-like low-level request capabilities via `http.Client`\r\n  - Higher-level API with `client.putStream()`, `client.getFile()`, etc.\r\n  - Copying and multi-file delete support\r\n  - Streaming file upload and direct stream-piping support\r\n\r\n## Examples\r\n\r\nThe following examples demonstrate some capabilities of knox and the S3 REST\r\nAPI. First things first, create an S3 client:\r\n\r\n```js\r\nvar client = knox.createClient({\r\n    key: '<api-key-here>'\r\n  , secret: '<secret-here>'\r\n  , bucket: 'learnboost'\r\n});\r\n```\r\n\r\nBy default knox will send all requests to the global endpoint\r\n(bucket.s3.amazonaws.com). This works regardless of the region where the bucket\r\nis. But if you want to manually set the endpoint (for performance reasons) you\r\ncan do it with the `endpoint` option.\r\n\r\n### PUT\r\n\r\nIf you want to directly upload some strings to S3, you can use the `Client#put`\r\nmethod with a string or buffer, just like you would for any `http.Client`\r\nrequest. You pass in the filename as the first parameter, some headers for the\r\nsecond, and then listen for a `'response'` event on the request. Then send the\r\nrequest using `req.end()`. If we get a 200 response, great!\r\n\r\n```js\r\nvar object = { foo: \"bar\" };\r\nvar string = JSON.stringify(object);\r\nvar req = client.put('/test/obj.json', {\r\n    'Content-Length': string.length\r\n  , 'Content-Type': 'application/json'\r\n});\r\nreq.on('response', function(res){\r\n  if (200 == res.statusCode) {\r\n    console.log('saved to %s', req.url);\r\n  }\r\n});\r\nreq.end(string);\r\n```\r\n\r\nBy default the _x-amz-acl_ header is _public-read_, meaning anyone can __GET__\r\nthe file. To alter this simply pass this header to the client request method.\r\n\r\n```js\r\nclient.put('/test/obj.json', { 'x-amz-acl': 'private' });\r\n```\r\n\r\nEach HTTP verb has an alternate method with the \"File\" suffix, for example\r\n`put()` also has a higher level method named `putFile()`, accepting a source\r\nfilename and performing the dirty work shown above for you. Here is an example\r\nusage:\r\n\r\n```js\r\nclient.putFile('my.json', '/user.json', function(err, res){\r\n  // Logic\r\n});\r\n```\r\n\r\nAnother alternative is to stream via `Client#putStream()`, for example:\r\n\r\n```js\r\nhttp.get('http://google.com/doodle.png', function(res){\r\n  var headers = {\r\n      'Content-Length': res.headers['content-length']\r\n    , 'Content-Type': res.headers['content-type']\r\n  };\r\n  client.putStream(res, '/doodle.png', headers, function(err, res){\r\n    // Logic\r\n  });\r\n});\r\n```\r\n\r\nAnd if you want a nice interface for putting a buffer or a string of data,\r\nuse `Client#putBuffer()`:\r\n\r\n```js\r\nvar buffer = new Buffer('a string of data');\r\nvar headers = {\r\n  'Content-Type': 'text/plain'\r\n};\r\nclient.putBuffer(buffer, '/string.txt', headers, function(err, res){\r\n  // Logic\r\n});\r\n```\r\n\r\nNote that both `putFile` and `putStream` will stream to S3 instead of reading\r\ninto memory, which is great. And they return objects that emit `'progress'`\r\nevents too, so you can monitor how the streaming goes! The progress events have\r\nfields `written`, `total`, and `percent`.\r\n\r\n### GET\r\n\r\nBelow is an example __GET__ request on the file we just shoved at S3. It simply\r\noutputs the response status code, headers, and body.\r\n\r\n```js\r\nclient.get('/test/Readme.md').on('response', function(res){\r\n  console.log(res.statusCode);\r\n  console.log(res.headers);\r\n  res.setEncoding('utf8');\r\n  res.on('data', function(chunk){\r\n    console.log(chunk);\r\n  });\r\n}).end();\r\n```\r\n\r\nThere is also `Client#getFile()` which uses a callback pattern instead of giving\r\nyou the raw request:\r\n\r\n```js\r\nclient.getFile('/test/Readme.md', function(err, res){\r\n  // Logic\r\n});\r\n```\r\n\r\n### DELETE\r\n\r\nDelete our file:\r\n\r\n```js\r\nclient.del('/test/Readme.md').on('response', function(res){\r\n  console.log(res.statusCode);\r\n  console.log(res.headers);\r\n}).end();\r\n```\r\n\r\nLikewise we also have `Client#deleteFile()` as a more concise (yet less\r\nflexible) solution:\r\n\r\n```js\r\nclient.deleteFile('/test/Readme.md', function(err, res){\r\n  // Logic\r\n});\r\n```\r\n\r\n### HEAD\r\n\r\nAs you might expect we have `Client#head` and `Client#headFile`, following the\r\nsame pattern as above.\r\n\r\n### Advanced Operations\r\n\r\nKnox supports a few advanced operations. Like copying files:\r\n\r\n```js\r\nclient.copy('/test/Readme.md', '/test/Readme.markdown').on('response', function(res){\r\n  console.log(res.statusCode);\r\n  console.log(res.headers);\r\n}).end();\r\n\r\n// or\r\n\r\nclient.copyFile('/test/Readme.md', '/test/Readme.markdown', function(err, res){\r\n  // Logic\r\n});\r\n```\r\n\r\nand deleting multiple files at once:\r\n\r\n```js\r\nclient.deleteMultiple(['/test/Readme.md', '/test/Readme.markdown'], function(err, res){\r\n  // Logic\r\n});\r\n```\r\n\r\nAnd you can always issue ad-hoc requests, e.g. the following to\r\n[get an object's ACL][acl]:\r\n\r\n```js\r\nclient.request('GET', '/test/Readme.md?acl').on('response', function(res){\r\n  // Read and parse the XML response.\r\n  // Everyone loves XML parsing.\r\n}).end();\r\n```\r\n\r\n[acl]: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectGETacl.html\r\n\r\n## Running Tests\r\n\r\nTo run the test suite you must first have an S3 account, and create\r\na file named _./auth_, which contains your credentials as json, for example:\r\n\r\n```json\r\n{\r\n  \"key\":\"<api-key-here>\",\r\n  \"secret\":\"<secret-here>\",\r\n  \"bucket\":\"<your-bucket-name>\"\r\n}\r\n```\r\n\r\nThen install the dev dependencies and execute the test suite:\r\n\r\n    $ npm install\r\n    $ npm test\r\n",
  "readmeFilename": "Readme.md",
  "_id": "knox@0.3.1",
  "_from": "knox@~0.3.1"
}
